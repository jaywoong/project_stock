### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
ğŸ“¦scraping
 â”£ ğŸ“‚notebook
 â”ƒ â”£ ğŸ“œbigkinds.ipynb
 â”ƒ â”£ ğŸ“œbigkinds_db_analog.ipynb
 â”ƒ â”£ ğŸ“œinvestingcom.ipynb
 â”ƒ â”£ ğŸ“œkrx.ipynb
 â”ƒ â”— ğŸ“œnaverESG.ipynb
 â”£ ğŸ“œbigkinds.db
 â”— ğŸ“œbigkinds_db_schedule.py
```



### ë°ì´í„° ìˆ˜ì§‘

- ë°ì´í„° ìˆ˜ì§‘ ëŒ€ìƒ

  `ì‚¼ì„±ì „ì, SKí•˜ì´ë‹‰ìŠ¤, LGí™”í•™, LGì „ì, LGì´ë…¸í…, ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤, ì‚¼ì„±ì „ê¸°, ì‚¼ì„±ìƒëª…, ì‚¼ì„±í™”ì¬, SKí…”ë ˆì½¤, KT, í˜„ëŒ€ê±´ì„¤, ì‚¼ì„±ì—”ì§€ë‹ˆì–´ë§, ëŒ€í•œí•­ê³µ, í˜„ëŒ€ì°¨, ê¸°ì•„, ì˜¤ë¦¬ì˜¨, CJì œì¼ì œë‹¹, ì˜¤ëšœê¸°, ë¯¸ë˜ì—ì…‹ëŒ€ìš°, í•œêµ­ê¸ˆìœµì§€ì£¼, NHíˆ¬ìì¦ê¶Œ, LGìƒí™œê±´ê°•, ì•„ëª¨ë ˆí¼ì‹œí”½, ì•„ëª¨ë ˆG, ê°•ì›ëœë“œ, í˜¸í…”ì‹ ë¼, KBê¸ˆìœµ, ì‹ í•œì§€ì£¼, í•˜ë‚˜ê¸ˆìœµì§€ì£¼, ë¡¯ë°ì‡¼í•‘, ì´ë§ˆíŠ¸, ì‹ ì„¸ê³„, GSë¦¬í…Œì¼, NAVER, ì¹´ì¹´ì˜¤, CJENM, ìŠ¤íŠœë””ì˜¤ë“œë˜ê³¤, ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤, ì…€íŠ¸ë¦¬ì˜¨, í•œë¯¸ì•½í’ˆ, ì—”ì”¨ì†Œí”„íŠ¸, ë„·ë§ˆë¸”, í•œí™”ì†”ë£¨ì…˜, LS, POSCO,ê³ ë ¤ì•„ì—°, S-Oil, SKì´ë…¸ë² ì´ì…˜, HMM`

  

- ë°ì´í„° ì¶œì²˜ ë° ìˆ˜ì§‘ ë°©ë²•

  - ê³µí†µ ê±°ì‹œ ê²½ì œ ì§€í‘œ ì¤‘ `S&P, CBOE` ëŠ” `DataFinanceReader` ëª¨ë“ˆì„ í†µí•´ ìˆ˜ì§‘í•˜ê³ ,

    `NASDAQ, futures2y, futures10y` ì€ [ì¸ë² ìŠ¤íŒ…ë‹·ì»´](https://kr.investing.com/indices/nasdaq-composite-historical-data)  ìŠ¤í¬ë˜í•‘ì„ í†µí•´ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.

  - ê°œë³„ì£¼ ê´€ë ¨ ì§€í‘œì¸ `ê±°ë˜ëŸ‰, atr, PER, PBR, ê¸°ê´€í•©ê³„, ê¸°íƒ€ë²•ì¸, ê°œì¸, ì™¸êµ­ì¸í•©ê³„` ëŠ” [KRX](http://data.krx.co.kr/contents/MDC/MAIN/main/index.cmd) ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” `pykrx` ëª¨ë“ˆì„ í†µí•´ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. 

  - [ë¹…ì¹´ì¸ì¦ˆ](https://www.bigkinds.or.kr/) ì—ì„œ ê¸°ì—…ë³„ ìµœê·¼ ì¼ì£¼ì¼ ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ìˆ˜ì§‘í•´ ì›¹ì—ì„œ ì œê³µí–ˆìŠµë‹ˆë‹¤.

  ```
  # scraping/bigkinds_db_schedule.py
  
  from selenium import webdriver
  from bs4 import BeautifulSoup
  class Bigkinds:
      def __init__(self):
          self.driver = webdriver.Chrome('C:\chromedriver.exe')
      def crawling(self, url, stockname):
       	while curPage <= self.totalPage:
              soup = BeautifulSoup(self.driver.page_source, 'html.parser')
              articles = soup.select('div.news-inner')
              print('Current Page : {}'.format(curPage))
              for article in articles:
                  title = article.select_one('span.title-elipsis').text.strip()
                  press = article.select_one('div.info > div > a.provider').text.strip()
                  category = article.select_one('div.info > div > span.bullet-keyword').text.strip()
                   date = article.select_one('div.info > p.name').text.strip()
                   self.contents.append([title, press, category, date])
   
  ```

  

  

### SQLite3 ì—°ë™

```
# ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ í•©ì³ sqlite3ì— ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤.
# modeling/stock_db_schedule.py

import sqlite3
class UpdateDB:
    def __init__(self):
        self.conn = sqlite3.connect('../modeling/stock.db')
        self.c = self.conn.cursor()  
        
   def saving(self):  
        self.df_merge = pd.merge(self.df_krx, self.df_invest, on='date')
        self.df_merge.to_sql('{}'.format(self.stockname), self.conn, 	if_exists='append')  
        self.conn.commit()

```

```
# ì €ì¥ëœ sqlite3ë¥¼ ì›¹ì—ì„œ í˜¸ì¶œí•˜ì˜€ìŠµë‹ˆë‹¤.
# stock/views.py

def news(request):
	result = dict()
	db_news = sqlite3.connect('bigkinds.db')
	db_news.row_factory = sqlite3.Row
	c = db_news.cursor()
	c.execute("select title, press, category, date, text from {}_news".format('CJENM'))
    data = c.fetchall()
    result['erows'] = data
    
    return render(request, 'news.html', result)
```
